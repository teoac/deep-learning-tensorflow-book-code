{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0813 23:04:37.070080 140119469496064 deprecation.py:323] From <ipython-input-1-2bf31ac605dd>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0813 23:04:37.071257 140119469496064 deprecation.py:323] From /home/teoac/solarisTutorial/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0813 23:04:37.072701 140119469496064 deprecation.py:323] From /home/teoac/solarisTutorial/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0813 23:04:37.274317 140119469496064 deprecation.py:323] From /home/teoac/solarisTutorial/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataOneHot/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 23:04:37.277030 140119469496064 deprecation.py:323] From /home/teoac/solarisTutorial/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W0813 23:04:37.319545 140119469496064 deprecation.py:323] From /home/teoac/solarisTutorial/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataOneHot/train-labels-idx1-ubyte.gz\n",
      "Extracting dataOneHot/t10k-images-idx3-ubyte.gz\n",
      "Extracting dataOneHot/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('dataOneHot/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_RMSProp = 0.02\n",
    "learning_rate_GradientDescent = 0.5\n",
    "num_epochs = 100\n",
    "batch_size = 256\n",
    "display_step = 5\n",
    "input_size = 784 # 28x28=784\n",
    "hidden1_size = 128\n",
    "hidden2_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder 구조 정의\n",
    "def build_autoencoder(x):\n",
    "    # encoding: 784->128->64\n",
    "    Wh_1 = tf.Variable(tf.random_normal([input_size, hidden1_size]))\n",
    "    bh_1 = tf.Variable(tf.random_normal([hidden1_size]))\n",
    "    H1_output = tf.nn.sigmoid(tf.matmul(x,Wh_1) + bh_1)\n",
    "    Wh_2 = tf.Variable(tf.random_normal([hidden1_size, hidden2_size]))\n",
    "    bh_2 = tf.Variable(tf.random_normal([hidden2_size]))\n",
    "    H2_output = tf.nn.sigmoid(tf.matmul(H1_output,Wh_2) + bh_2)\n",
    "    # decoding: 64->128->784\n",
    "    Wh_3 = tf.Variable(tf.random_normal([hidden2_size, hidden1_size]))\n",
    "    bh_3 = tf.Variable(tf.random_normal([hidden1_size]))\n",
    "    H3_output = tf.nn.sigmoid(tf.matmul(H2_output,Wh_3) + bh_3)\n",
    "    Wo = tf.Variable(tf.random_normal([hidden1_size, input_size]))\n",
    "    bo = tf.Variable(tf.random_normal([input_size]))\n",
    "    X_reconstructed = tf.nn.sigmoid(tf.matmul(H3_output,Wo) + bo)\n",
    "    \n",
    "    return X_reconstructed, H2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax 분류기 정의\n",
    "def build_softmax_classification(x):\n",
    "    W_softmax = tf.Variable(tf.zeros([hidden2_size, 10])) # input_size 대신 hidden2_size. 인코딩된 값을 입력받을 것이므로...\n",
    "    b_softmax = tf.Variable(tf.zeros([10]))\n",
    "    y_pred = tf.nn.softmax(tf.matmul(x, W_softmax) + b_softmax)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, extracted_features = build_autoencoder(x)\n",
    "y_true = x\n",
    "y_pred_softmax = build_softmax_classification(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 23:04:37.723393 140119469496064 deprecation.py:323] From /home/teoac/solarisTutorial/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0813 23:04:37.800377 140119469496064 deprecation.py:506] From /home/teoac/solarisTutorial/lib/python3.5/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# 1: Pre-Training: MNIST 데이터 재구축을 목적으로 하는 손실 함수와 옵티마이저를 정의합니다.\n",
    "pretraining_loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "pretraining_train_step = tf.train.RMSPropOptimizer(learning_rate_RMSProp).minimize(pretraining_loss)\n",
    "\n",
    "# 2: Fine-Tuning: MNIST 데이터 분류를 목적으로하는 손실 함수와 옵티마이저를 정의합니다.\n",
    "finetuning_loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_pred_softmax), reduction_indices=[1])) # cross-entropy loss 함수\n",
    "finetuning_train_step = tf.train.GradientDescentOptimizer(learning_rate_GradientDescent).minimize(finetuning_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Pre-training Loss: 0.191352\n",
      "Epoch: 6, Pre-training Loss: 0.072684\n",
      "Epoch: 11, Pre-training Loss: 0.059698\n",
      "Epoch: 16, Pre-training Loss: 0.054612\n",
      "Epoch: 21, Pre-training Loss: 0.047236\n",
      "Epoch: 26, Pre-training Loss: 0.043226\n",
      "Epoch: 31, Pre-training Loss: 0.038424\n",
      "Epoch: 36, Pre-training Loss: 0.036469\n",
      "Epoch: 41, Pre-training Loss: 0.034315\n",
      "Epoch: 46, Pre-training Loss: 0.030423\n",
      "Epoch: 51, Pre-training Loss: 0.030959\n",
      "Epoch: 56, Pre-training Loss: 0.027994\n",
      "Epoch: 61, Pre-training Loss: 0.026473\n",
      "Epoch: 66, Pre-training Loss: 0.027474\n",
      "Epoch: 71, Pre-training Loss: 0.024613\n",
      "Epoch: 76, Pre-training Loss: 0.022621\n",
      "Epoch: 81, Pre-training Loss: 0.020996\n",
      "Epoch: 86, Pre-training Loss: 0.020147\n",
      "Epoch: 91, Pre-training Loss: 0.019124\n",
      "Epoch: 96, Pre-training Loss: 0.018463\n",
      "Step 1: MNIST 데이터 재구축을 위한 오토인코더 최적화 완료\n",
      "Epoch: 1, Fine-tuning Loss: 0.446166\n",
      "Epoch: 6, Fine-tuning Loss: 0.239878\n",
      "Epoch: 11, Fine-tuning Loss: 0.202973\n",
      "Epoch: 16, Fine-tuning Loss: 0.168865\n",
      "Epoch: 21, Fine-tuning Loss: 0.122659\n",
      "Epoch: 26, Fine-tuning Loss: 0.135495\n",
      "Epoch: 31, Fine-tuning Loss: 0.135648\n",
      "Epoch: 36, Fine-tuning Loss: 0.090414\n",
      "Epoch: 41, Fine-tuning Loss: 0.106022\n",
      "Epoch: 46, Fine-tuning Loss: 0.081684\n",
      "Epoch: 51, Fine-tuning Loss: 0.088339\n",
      "Epoch: 56, Fine-tuning Loss: 0.069167\n",
      "Epoch: 61, Fine-tuning Loss: 0.046000\n",
      "Epoch: 66, Fine-tuning Loss: 0.044381\n",
      "Epoch: 71, Fine-tuning Loss: 0.072447\n",
      "Epoch: 76, Fine-tuning Loss: 0.083555\n",
      "Epoch: 81, Fine-tuning Loss: 0.020180\n",
      "Epoch: 86, Fine-tuning Loss: 0.058760\n",
      "Epoch: 91, Fine-tuning Loss: 0.082607\n",
      "Epoch: 96, Fine-tuning Loss: 0.030753\n",
      "Epoch: 101, Fine-tuning Loss: 0.029567\n",
      "Epoch: 106, Fine-tuning Loss: 0.025429\n",
      "Epoch: 111, Fine-tuning Loss: 0.047232\n",
      "Epoch: 116, Fine-tuning Loss: 0.017990\n",
      "Epoch: 121, Fine-tuning Loss: 0.033394\n",
      "Epoch: 126, Fine-tuning Loss: 0.021200\n",
      "Epoch: 131, Fine-tuning Loss: 0.051335\n",
      "Epoch: 136, Fine-tuning Loss: 0.020992\n",
      "Epoch: 141, Fine-tuning Loss: 0.023906\n",
      "Epoch: 146, Fine-tuning Loss: 0.021317\n",
      "Epoch: 151, Fine-tuning Loss: 0.018630\n",
      "Epoch: 156, Fine-tuning Loss: 0.011334\n",
      "Epoch: 161, Fine-tuning Loss: 0.021112\n",
      "Epoch: 166, Fine-tuning Loss: 0.011309\n",
      "Epoch: 171, Fine-tuning Loss: 0.018020\n",
      "Epoch: 176, Fine-tuning Loss: 0.011989\n",
      "Epoch: 181, Fine-tuning Loss: 0.013523\n",
      "Epoch: 186, Fine-tuning Loss: 0.008588\n",
      "Epoch: 191, Fine-tuning Loss: 0.012511\n",
      "Epoch: 196, Fine-tuning Loss: 0.012536\n",
      "Step 2: MNIST 데이터 분류를 위한 오토인코더+softmax 분류기 최적화 완료\n",
      "Accuracy: 0.959100\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    # step1: MNIST 데이터 재구축을 위한 오토인코더 최적화 (pre-training)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, pretraining_loss_print = sess.run([pretraining_train_step, pretraining_loss], feed_dict={x: batch_xs})\n",
    "        if epoch%display_step == 0:\n",
    "            print('Epoch: %d, Pre-training Loss: %f' %((epoch+1), pretraining_loss_print))\n",
    "    print('Step 1: MNIST 데이터 재구축을 위한 오토인코더 최적화 완료')\n",
    "    \n",
    "    # step2: MNIST 데이터 분류를 위한 오토인코더+Softmax 분류기 최적화 (fine-tuning)\n",
    "    for epoch in range(num_epochs+100):\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, finetuning_loss_print = sess.run([finetuning_train_step, finetuning_loss], feed_dict={x: batch_xs, y:batch_ys})\n",
    "        if epoch%display_step == 0:\n",
    "            print('Epoch: %d, Fine-tuning Loss: %f' %((epoch+1), finetuning_loss_print))\n",
    "    print('Step 2: MNIST 데이터 분류를 위한 오토인코더+softmax 분류기 최적화 완료')\n",
    "    \n",
    "    # step3: 정확도 출력\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_pred_softmax,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('Accuracy: %f' %sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
